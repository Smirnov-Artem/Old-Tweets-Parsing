{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Parsing part\n",
    "2. Data processing part\n",
    "3. Jaccard similarity for tweets classification\n",
    "4. Word frequency analysis\n",
    "5. Tweets clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of the packages can be helpful in this project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "import re\n",
    "import en_core_web_sm\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd \n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn import metrics\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Parsing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"5_twitterBBC.csv\"\n",
    "f = open(file, \"w\")\n",
    "Headers = \"tweet_user, tweet_text,  replies,  retweets\\n\"\n",
    "f.write(Headers)\n",
    "url = \"https://twitter.com/BBCWorld\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the tweet\n",
    "tweets = soup.find_all(\"li\", attrs={\"class\":\"js-stream-item\"})\n",
    "\n",
    "# Writes tweet fetched in file\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        if tweet.find('p',{\"class\":'tweet-text'}):\n",
    "            tweet_user = tweet.find('span',{\"class\":'username'}).text.strip()\n",
    "            tweet_text = tweet.find('p',{\"class\":'tweet-text'}).text.encode('utf8').strip()\n",
    "            replies = tweet.find('span',{\"class\":\"ProfileTweet-actionCount\"}).text.strip()\n",
    "            retweets = tweet.find('span', {\"class\" : \"ProfileTweet-action--retweet\"}).text.strip()\n",
    "            # String interpolation technique\n",
    "            f.write(f'{tweet_user},/^{tweet_text}$/,{replies},{retweets}\\n')\n",
    "    except: AttributeError\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= '...'\n",
    "consumer_secret= '...'\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tw.API(auth)\n",
    "os.environ['TOKEN'] = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"xbox lang:rus\"\n",
    "max_results = 100\n",
    "start_time = \"2022-03-23T00:00:00.000Z\"\n",
    "key_word = 'Россия' #Russia in Russian\n",
    "\n",
    "def current_time(end):\n",
    "    #Inputs for the request\n",
    "    end_time = end\n",
    "    return (end_time)\n",
    "\n",
    "def create_df(end):\n",
    "    list_of_lists = []\n",
    "    json_response = json(end)\n",
    "    for i in range(0,len(json_response['data'])):\n",
    "        list_of_lists.append([json_response['data'][i]['created_at'], json_response['data'][i]['public_metrics'].get('retweet_count'), json_response['data'][i]['public_metrics'].get('reply_count'), json_response['data'][i]['public_metrics'].get('like_count'), json_response['data'][i]['public_metrics'].get('quote_count'), json_response['data'][i]['text']])\n",
    "    \n",
    "    df_temporary = pd.DataFrame(list_of_lists, columns=['created_at', 'retweet_count', 'reply_count', 'like_count', 'quote_count', 'text'])\n",
    "    end = df_temporary['created_at'].iloc[-1]\n",
    "    return df_temporary, end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = \"2022-03-24T23:59:00.000Z\"\n",
    "df1, end = create_df(end)\n",
    "q=0\n",
    "dataframes_list = []\n",
    "\n",
    "while(q<500):\n",
    "    df2, end = create_df(end)\n",
    "    dataframes_list.append(df2)\n",
    "    q+=1\n",
    "    \n",
    "df_ = pd.concat(dataframes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data processing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasettime = []\n",
    "for i in range(0,len(df_)):\n",
    "    orig_time=df_['created_at'].iloc[i][:-5]\n",
    "    d = datetime.datetime.strptime(orig_time, '%Y-%m-%dT%H:%M:%S')\n",
    "    datasettime.append(d)\n",
    "    \n",
    "datasettime = pd.DataFrame(datasettime)\n",
    "datasettime.rename(\n",
    "    columns={0:\"time_stamp\"}\n",
    "          ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasettime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df_, datasettime], axis=1)\n",
    "result = result.drop(['created_at'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the hashtags, mentions and unwanted characters.\n",
    "def clean_text(result, text):\n",
    "    result[text]=result[text].apply(str)\n",
    "    result[text] = result[text].str.lower()\n",
    "    result[text] = result[text].apply(lambda elem: re.sub(r\" rt : @[^\\s]+|@[^\\s]+|@[A-Za-z0–9]+|@[0–9]|(@[A-Za-z0–9]+)|\\n|rt|(@[A-Za-z0–9]+)|http.+|://+\", \"\",   elem)) \n",
    "    return result\n",
    "\n",
    "result = clean_text(result, \"text\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.index = result['time_stamp']\n",
    "result['tweet'] = result['text']\n",
    "result = result.drop(['text'], axis = 1)\n",
    "result = result.drop(['created_at'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used here functons from https://github.com/ada-k/TweetsClassification/blob/master/analysis.ipynb\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "nlp = en_core_web_sm.load() \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "\n",
    "def furnished(text):\n",
    "    final_text = []\n",
    "    for i in w_tokenizer.tokenize(text):\n",
    "       if i.lower() not in stop:\n",
    "           word = lemmatizer.lemmatize(i)\n",
    "           final_text.append(word.lower())\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3  Jaccard similarity for tweets classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a premise that there are 3 potential types of comments: negative, neutral and positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing some of the tweets, I identified a few words-markers of the author's mood. I will not specify the words I used because of ethical reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part I used Jaccard similarity for tweets classification. This is the resource I relied on https://medium.com/swlh/tweets-classification-and-clustering-in-python-b107be1ba7c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_related_words = '''...'''\n",
    "positive_related_words = '''...'''\n",
    "neutral_related_words = '''...'''\n",
    "\n",
    "negative = furnished(negative_related_words)\n",
    "positive = furnished(positive_related_words)\n",
    "neutral = furnished(neutral_related_words)\n",
    "\n",
    "df_.text = df_.text.apply(furnished)\n",
    "\n",
    "s = negative\n",
    "words = s.split()\n",
    "negatives = \" \".join(sorted(set(words), key=words.index))\n",
    "\n",
    "s = positive\n",
    "words = s.split()\n",
    "positive = \" \".join(sorted(set(words), key=words.index))\n",
    "\n",
    "s = neutral\n",
    "words = s.split()\n",
    "neutral = \" \".join(sorted(set(words), key=words.index))\n",
    "neutral\n",
    "\n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = TfidfVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "def get_scores(group,tweets):\n",
    "    scores = []\n",
    "    for tweet in tweets:\n",
    "        s = jaccard_similarity(group, tweet)\n",
    "        scores.append(s)\n",
    "    return scores\n",
    "\n",
    "negative_scores = get_scores(negative, df_.text.to_list())\n",
    "positive_scores = get_scores(positive, df_.text.to_list())\n",
    "neutral_scores = get_scores(neutral, df_.text.to_list())\n",
    "\n",
    "negative_scores = pd.DataFrame(negative_scores, columns=['Negative'])\n",
    "positive_scores = pd.DataFrame(positive_scores, columns=['Positive'])\n",
    "neutral_scores = pd.DataFrame(neutral_scores, columns=['Neutral'])\n",
    "\n",
    "frames = pd.concat([negative_scores, positive_scores, neutral_scores], axis=1)\n",
    "\n",
    "df_ = pd.concat([df_, frames], axis=1)\n",
    "\n",
    "df_without_text = df_.drop('text', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.concat([pd.DataFrame(positive_scores), pd.DataFrame(neutral_scores), pd.DataFrame(negative_scores)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(l1, l2, l3):\n",
    "    pos = []\n",
    "    neu = []\n",
    "    neg = []\n",
    "\n",
    "    for i, j, k in zip(l1, l2, l3):\n",
    "        m = max(i, j, k)\n",
    "        if m == i:\n",
    "            pos.append(1)\n",
    "        else:\n",
    "            pos.append(0)\n",
    "        if m == j:\n",
    "            neu.append(1)\n",
    "        else:\n",
    "            neu.append(0)        \n",
    "        if m == k:\n",
    "            neg.append(1)\n",
    "        else:\n",
    "            neg.append(0)   \n",
    "            \n",
    "    return pos, neu, neg\n",
    "l1 = scores_df.Positive.to_list()\n",
    "l2 = scores_df.Neutral.to_list()\n",
    "l3 = scores_df.Negative.to_list()\n",
    "\n",
    "pos, neu, neg = get_classes(l1, l2, l3)\n",
    "data = {'pos':pos, 'neu':neu, 'neg':neg}\n",
    "class_df = pd.DataFrame(data)\n",
    "new_groups_df = class_df.sum()\n",
    "new_groups_df['total'] = new_groups_df['pos'] + new_groups_df['neu'] + new_groups_df['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groups_df['total'] = new_groups_df['pos'] + new_groups_df['neu'] + new_groups_df['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of tweets classification using Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'Positive', 'Neutral', 'Negative'\n",
    "sizes = [new_groups_df.pos, new_groups_df.neu, new_groups_df.neg]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Positive','Neutral','Negative']\n",
    "values = [new_groups_df.pos, new_groups_df.neu, new_groups_df.neg]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n",
    "                             insidetextorientation='radial', hole=.3\n",
    "                            )])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Word frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I would like to analyze the frequency of certain words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "words_all_list = []\n",
    "w = []\n",
    "\n",
    "for i in range(0,len(result)):\n",
    "    words.append(result[\"text\"].iloc[i])\n",
    "    \n",
    "for i in range(0,len(words)):\n",
    "    words_all_list.append(words[i].split(\" \"))\n",
    "\n",
    "for i in range(0,len(words_all_list)):\n",
    "    for j in range(0,len(words_all_list[i])):\n",
    "        w.append(words_all_list[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = w\n",
    "unique, counts = np.unique(x, return_counts=True)\n",
    "\n",
    "a = pd.DataFrame((unique, counts)).T\n",
    "a = a.sort_values(by=1, ascending=False)\n",
    "a = a.rename(columns={0: \"Word\", 1: \"Freq\"})\n",
    "a = a.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger the word size, the more common it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {w: f for w, f in\n",
    "     zip(np.array(a[\"Word\"][21:500]),\n",
    "         np.array(a[\"Freq\"][21:500]))}\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200, background_color='black', prefer_horizontal=1)\n",
    "wordcloud.generate_from_frequencies(frequencies=d)\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask3 = np.array(Image.open('file.jpeg'))\n",
    "\n",
    "image_colors = ImageColorGenerator(mask3)\n",
    "\n",
    "d = {w: f for w, f in\n",
    "     zip(np.array(a[\"Word\"][21:1000]),\n",
    "         np.array(a[\"Freq\"][21:1000]))}\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200, random_state=100, max_words=1000, background_color='white', prefer_horizontal=1, mask=mask3)\n",
    "wordcloud.generate_from_frequencies(frequencies=d)\n",
    "plt.figure(figsize=(180,15))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = pd.DataFrame(data)\n",
    "# Considering 3 grams and mimnimum frq as 0\n",
    "tf_idf_vect = CountVectorizer(analyzer='word',ngram_range=(1,1), min_df = 0.0001)\n",
    "tf_idf_vect.fit(result['text'])\n",
    "desc_matrix = tf_idf_vect.transform(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Tweets clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I use the kMeans method for data clustering (unsupervised method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(desc_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "tweets = {'Tweet': result[\"text\"].tolist(), 'Cluster': clusters}\n",
    "frame = pd.DataFrame(tweets, index = [clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found common patterns within each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Furious and obscene', 'Negative restrained','Sarcastic, skeptical and meme']\n",
    "values = frame['Cluster'].value_counts()[0], frame['Cluster'].value_counts()[1], frame['Cluster'].value_counts()[2]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Negative restrained','Sarcastic, skeptical and meme','Furious and obscene']\n",
    "values = frame['Cluster'].value_counts()\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n",
    "                             insidetextorientation='radial', hole=.3\n",
    "                            )])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
